#importing the required libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings("ignore")

df = pd.read_csv("/content/bank-additional-full.csv",sep=";")

#sep is a parameter that specifies that the separator is a semicolon.

df

It contains information related to direct marketing campaigns conducted by a
Portuguese banking institution.

The goal of using this dataset is often to build a predictive model
that can accurately predict whether a client will subscribe to a
term deposit based on the available attributes.

df["y"].value_counts()

df["y"].unique()

df["y"].nunique()

# EDA: exploratory data analysis

df.head()

df.tail()

df.shape

df.info()

df.describe()

df.describe(include = "all")

df.isnull()

df.isnull().sum()

df.isnull().sum().sum()

df.dropna(inplace=True)

df

df.isnull().sum()

df.columns

df["pdays"].unique()

# outliers detection

#removing the outliers from the numerical columns and plotting the boxplot.
for col in df.columns:
  if((df[col].dtype=="int64") or (df[col].dtype=="float64")):
    sns.boxplot(df[col])
    plt.xlabel(col)
    plt.ylabel("count")
    plt.show()

# outliers removal

out_list = ["age","duration","campaign","cons.conf.idx"]

for i in out_list:
  Q1 = df[i].quantile(0.25)
  Q3 = df[i].quantile(0.75)


  IQR = Q3 - Q1
  df = df[(df[i]>=Q1-1.5*IQR) & (df[i]<=Q3+1.5*IQR)]


#outliers detection

for col in df.columns:
    if ((df[col].dtype == "int64") or (df[col].dtype == "float64")):
        sns.boxplot(df[col])
        plt.xlabel(col)
        plt.ylabel('count')
        plt.show()

#feature selection

df.duplicated().sum()


df = df.drop_duplicates()

df

df["job"].unique()

# encoding: converting categorical data into numerical data

# one hot encoding ----------> pd.get_dummies()
# label encoding---------> label encoder

data1 = pd.get_dummies(df)

data1

x = df.iloc[:,:-1]   #excluding target column
y = df.iloc[:,-1]

x

y

# lets convert the categorical data to numerical data
#label encoding

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()

for i in x.columns:
  if x[i].dtype == "object":
    x[i] = le.fit_transform(x[i])
    print(le.classes_)

x

# # feature selection : feature selection is a process which is used to select most relevent and important
# #      features which can be used ahead for the model building

# 1) vif --------> variance inflation factor( vif value for the columns less than 6 or 5 is considered)
# 2) IV ---------> Information values, classifies my independent features into a week,best,avg etc categories
# 3) RFE --------> Recursive Feature elimination identifies a dataset's key features
# 4) Correlation --------> independependent features that shows a correlation values greater than 0.5 with the dependent
#        feature is considered.

# RFE:

from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()

# rfe = RFE(model, n_features_to_select)

rfe = RFE(model)

rfe.fit(x,y)

rfe.support_

# returns the index values of the colummns that are important
selected_indices = [i for i,data in enumerate(rfe.support_) if data == True]
selected_indices

column_names = x.columns
selected_column_name = [column_names[i] for i in selected_indices]
selected_column_name

x = x.loc[:,['marital',
 'education',
 'default',
 'contact',
 'month',
 'day_of_week',
 'campaign',
 'previous',
 'cons.conf.idx',
 'euribor3m']]

x


y

#Next step train test
from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test = train_test_split(x,y,train_size = 0.75, random_state = 99)
#random state is very important to train the model
#Train size here refers to the size of the dataset used for training the model

x_train

x_test

y_test

y_train

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()

lr.fit(x_train,y_train)

y_pred = lr.predict(x_test)

y_pred #predicting the target column

#comparing the predicted data with the actual data
from sklearn.metrics import accuracy_score,confusion_matrix

accuracy_score(y_test,y_pred)

confusion_matrix(y_test,y_pred)

8098+65

8098+48+581+65

8163/8792

# recall = tp/tp+fn  -------> sensitivity or true positive rate
# recall measures the ability of a model to correctly identify all positive instances in a datase
8098+48

8098/8146

"""
specificity is the true negative rate
F1 score = 2*(precision*recall)/(precision + recall)
"""

from sklearn.metrics import classification_report

c_report = classification_report(y_test,y_pred)

print(c_report)

